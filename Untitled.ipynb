{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9926d4db70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Iterator, List, Dict\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, SequenceLabelField,LabelField,ArrayField, ListField\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer, TokenCharactersIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.nn.util import get_text_field_mask, masked_log_softmax\n",
    "from allennlp.training.metrics import Average\n",
    "from allennlp.data.iterators import BucketIterator, BasicIterator\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "from torch.nn.modules import NLLLoss\n",
    "from torch.nn import LogSoftmax\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "name_category_list = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    lines = readLines(filename)\n",
    "    for name in lines:\n",
    "        name_category_list.append((name,category))\n",
    "\n",
    "X = [x[0] for x in name_category_list]\n",
    "y = [x[1] for x in name_category_list]\n",
    "x_train, x_val, y_train, y_val = train_test_split(X,y,test_size=.2, stratify=y)\n",
    "with open('train_data.txt','w+') as f:\n",
    "    f.write('\\n'.join('{} {}'.format(x[0],x[1]) for x in zip(x_train,y_train)))\n",
    "with open('val_data.txt','w+') as f:\n",
    "    f.write('\\n'.join('{} {}'.format(x[0],x[1]) for x in zip(x_val,y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameLangDatasetReader(DatasetReader):\n",
    "\n",
    "    def __init__(self, token_indexers: Dict[str, TokenIndexer] = None) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.token_indexers = token_indexers or {\"characters\": SingleIdTokenIndexer()}\n",
    "        \n",
    "    def text_to_instance(self, name: str, label: str=None) -> Instance:\n",
    "        tokens = [Token(ch) for ch in name]\n",
    "        char_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"name\": char_field}\n",
    "        if label is None:\n",
    "            return Instance(fields)\n",
    "        \n",
    "        label_field = LabelField(label=label)\n",
    "        fields[\"label\"] = label_field\n",
    "        return Instance(fields)\n",
    "    \n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        with open(file_path) as f:\n",
    "            for line in f:\n",
    "                line = line.strip().split()\n",
    "                language = line[-1]\n",
    "                name = ' '.join(line[:-1])\n",
    "                yield self.text_to_instance(name, language)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16059it [00:00, 43296.89it/s]\n",
      "4015it [00:00, 71876.37it/s]\n",
      "100%|██████████| 20074/20074 [00:00<00:00, 172801.07it/s]\n"
     ]
    }
   ],
   "source": [
    "reader = NameLangDatasetReader()\n",
    "train_dataset = reader.read('train_data.txt')\n",
    "validation_dataset = reader.read('val_data.txt')\n",
    "vocab = Vocabulary.from_instances(train_dataset + validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([M, u, h, l, f, e, l, d], 'German')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0].fields['name'].tokens, train_dataset[0].fields['label'].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NamesClassifier(Model):\n",
    "    def __init__(self,\n",
    "                 char_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 vocab: Vocabulary) -> None:\n",
    "        super().__init__(vocab)\n",
    "        self.char_embeddings = char_embeddings\n",
    "        self.encoder = encoder\n",
    "        self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                          out_features=vocab.get_vocab_size('labels'))\n",
    "        self.accuracy = Average()\n",
    "        self.m = LogSoftmax() # Softmax try and see\n",
    "        self.loss = NLLLoss()\n",
    "        \n",
    "    def forward(self,\n",
    "                name: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
    "        mask = get_text_field_mask(name)\n",
    "        embeddings = self.char_embeddings(name)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        tag_logits = self.hidden2tag(encoder_out)\n",
    "        output = {\"tag_logits\": tag_logits}\n",
    "        if label is not None:\n",
    "            output[\"loss\"] = self.loss(self.m(tag_logits), label)\n",
    "            prediction = tag_logits.max(1)[1]\n",
    "            self.accuracy(prediction.eq(label).double().mean())\n",
    "        return output\n",
    "    \n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from overrides import overrides\n",
    "\n",
    "from allennlp.common.util import JsonDict\n",
    "from allennlp.data import DatasetReader, Instance\n",
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from allennlp.models import Model\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "class NamePredictor(Predictor):\n",
    "\n",
    "    def __init__(self, model: Model, dataset_reader: DatasetReader, language: str = 'en_core_web_sm') -> None:\n",
    "        super().__init__(model, dataset_reader)\n",
    "\n",
    "    def predict(self, name: str) -> JsonDict:\n",
    "        tag_logits = self.predict_json({\"name\" : name})['tag_logits']\n",
    "        print(tag_logits)\n",
    "        max_id = np.argmax(tag_logits, axis=-1)\n",
    "        return model.vocab.get_token_from_index(max_id, 'labels')\n",
    "        \n",
    "    @overrides\n",
    "    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
    "        name = json_dict[\"name\"]\n",
    "        return self._dataset_reader.text_to_instance(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8030 [00:00<?, ?it/s]/home/enes/anaconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "accuracy: 0.6150, loss: 1.2857 ||: 100%|██████████| 8030/8030 [00:18<00:00, 431.42it/s]\n",
      "accuracy: 0.6877, loss: 1.0660 ||: 100%|██████████| 2008/2008 [00:01<00:00, 1082.39it/s]\n",
      "  0%|          | 0/8030 [00:00<?, ?it/s]/home/enes/anaconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "accuracy: 0.6937, loss: 1.0317 ||: 100%|██████████| 8030/8030 [00:18<00:00, 435.26it/s]\n",
      "accuracy: 0.7099, loss: 0.9925 ||: 100%|██████████| 2008/2008 [00:01<00:00, 1119.62it/s]\n",
      "  0%|          | 0/8030 [00:00<?, ?it/s]/home/enes/anaconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "accuracy: 0.7152, loss: 0.9632 ||: 100%|██████████| 8030/8030 [00:18<00:00, 442.81it/s]\n",
      "accuracy: 0.7171, loss: 0.9763 ||: 100%|██████████| 2008/2008 [00:01<00:00, 1089.31it/s]\n",
      "  0%|          | 0/8030 [00:00<?, ?it/s]/home/enes/anaconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "accuracy: 0.7238, loss: 0.9338 ||: 100%|██████████| 8030/8030 [00:18<00:00, 440.35it/s]\n",
      "accuracy: 0.7179, loss: 0.9615 ||: 100%|██████████| 2008/2008 [00:01<00:00, 1119.47it/s]\n",
      "  0%|          | 0/8030 [00:00<?, ?it/s]/home/enes/anaconda3/envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "accuracy: 0.7315, loss: 0.9171 ||: 100%|██████████| 8030/8030 [00:18<00:00, 444.52it/s]\n",
      "accuracy: 0.7328, loss: 0.9254 ||: 100%|██████████| 2008/2008 [00:01<00:00, 1121.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 4,\n",
       " 'peak_cpu_memory_MB': 2354.14,\n",
       " 'peak_gpu_0_memory_MB': 752,\n",
       " 'training_duration': '00:01:40',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 4,\n",
       " 'epoch': 4,\n",
       " 'training_accuracy': tensor(0.7315, dtype=torch.float64),\n",
       " 'training_loss': 0.9171358023307392,\n",
       " 'training_cpu_memory_MB': 2354.14,\n",
       " 'training_gpu_0_memory_MB': 752,\n",
       " 'validation_accuracy': tensor(0.7328, dtype=torch.float64),\n",
       " 'validation_loss': 0.9253557659240358,\n",
       " 'best_validation_accuracy': tensor(0.7328, dtype=torch.float64),\n",
       " 'best_validation_loss': 0.9253557659240358}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'), embedding_dim=EMBEDDING_DIM)\n",
    "char_embeddings = BasicTextFieldEmbedder({\"characters\": token_embedding})\n",
    "lstm = PytorchSeq2VecWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "model = NamesClassifier(char_embeddings, lstm, vocab)\n",
    "if torch.cuda.is_available():\n",
    "    cuda_device = 0\n",
    "    model = model.cuda(cuda_device)\n",
    "else:\n",
    "    cuda_device = -1\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-3, weight_decay=1e-4)\n",
    "iterator = BasicIterator(batch_size=2)\n",
    "iterator.index_with(vocab)\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=validation_dataset,\n",
    "                  patience=10,\n",
    "                  num_epochs=5,\n",
    "                  cuda_device=cuda_device)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.55558443069458, -1.1911295652389526, -6.244527816772461, -2.878906726837158, -1.9350042343139648, -2.196650743484497, 0.3870698809623718, -1.3938158750534058, -2.520087957382202, -2.6504149436950684, -6.677775859832764, -2.172342538833618, -2.7314114570617676, -0.6685163378715515, -3.9820199012756348, -5.869513511657715, -3.290464162826538, -4.328010559082031]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Russian'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor = NamePredictor(model, dataset_reader=reader)\n",
    "predictor.predict(\"Vikhorev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----Vocabulary Statistics----\n",
      "\n",
      "\n",
      "Top 10 most frequent tokens in namespace 'tokens':\n",
      "\tToken: a\t\tFrequency: 14767\n",
      "\tToken: o\t\tFrequency: 10802\n",
      "\tToken: e\t\tFrequency: 10317\n",
      "\tToken: i\t\tFrequency: 10202\n",
      "\tToken: n\t\tFrequency: 9348\n",
      "\tToken: r\t\tFrequency: 7511\n",
      "\tToken: h\t\tFrequency: 6547\n",
      "\tToken: s\t\tFrequency: 6511\n",
      "\tToken: l\t\tFrequency: 5950\n",
      "\tToken: k\t\tFrequency: 5894\n",
      "\n",
      "Top 10 longest tokens in namespace 'tokens':\n",
      "\tToken: a\t\tlength: 1\tFrequency: 14767\n",
      "\tToken: o\t\tlength: 1\tFrequency: 10802\n",
      "\tToken: e\t\tlength: 1\tFrequency: 10317\n",
      "\tToken: i\t\tlength: 1\tFrequency: 10202\n",
      "\tToken: n\t\tlength: 1\tFrequency: 9348\n",
      "\tToken: r\t\tlength: 1\tFrequency: 7511\n",
      "\tToken: h\t\tlength: 1\tFrequency: 6547\n",
      "\tToken: s\t\tlength: 1\tFrequency: 6511\n",
      "\tToken: l\t\tlength: 1\tFrequency: 5950\n",
      "\tToken: k\t\tlength: 1\tFrequency: 5894\n",
      "\n",
      "Top 10 shortest tokens in namespace 'tokens':\n",
      "\tToken: ,\t\tlength: 1\tFrequency: 3\n",
      "\tToken: X\t\tlength: 1\tFrequency: 14\n",
      "\tToken: q\t\tlength: 1\tFrequency: 38\n",
      "\tToken: x\t\tlength: 1\tFrequency: 59\n",
      "\tToken: Q\t\tlength: 1\tFrequency: 60\n",
      "\tToken: '\t\tlength: 1\tFrequency: 87\n",
      "\tToken:  \t\tlength: 1\tFrequency: 115\n",
      "\tToken: U\t\tlength: 1\tFrequency: 142\n",
      "\tToken: I\t\tlength: 1\tFrequency: 235\n",
      "\tToken: W\t\tlength: 1\tFrequency: 332\n",
      "\n",
      "Top 10 most frequent tokens in namespace 'labels':\n",
      "\tToken: Russian\t\tFrequency: 9408\n",
      "\tToken: English\t\tFrequency: 3668\n",
      "\tToken: Arabic\t\tFrequency: 2000\n",
      "\tToken: Japanese\t\tFrequency: 991\n",
      "\tToken: German\t\tFrequency: 724\n",
      "\tToken: Italian\t\tFrequency: 709\n",
      "\tToken: Czech\t\tFrequency: 519\n",
      "\tToken: Spanish\t\tFrequency: 298\n",
      "\tToken: Dutch\t\tFrequency: 297\n",
      "\tToken: French\t\tFrequency: 277\n",
      "\n",
      "Top 10 longest tokens in namespace 'labels':\n",
      "\tToken: Portuguese\t\tlength: 10\tFrequency: 74\n",
      "\tToken: Vietnamese\t\tlength: 10\tFrequency: 73\n",
      "\tToken: Japanese\t\tlength: 8\tFrequency: 991\n",
      "\tToken: Scottish\t\tlength: 8\tFrequency: 100\n",
      "\tToken: Russian\t\tlength: 7\tFrequency: 9408\n",
      "\tToken: English\t\tlength: 7\tFrequency: 3668\n",
      "\tToken: Italian\t\tlength: 7\tFrequency: 709\n",
      "\tToken: Spanish\t\tlength: 7\tFrequency: 298\n",
      "\tToken: Chinese\t\tlength: 7\tFrequency: 268\n",
      "\tToken: Arabic\t\tlength: 6\tFrequency: 2000\n",
      "\n",
      "Top 10 shortest tokens in namespace 'labels':\n",
      "\tToken: Greek\t\tlength: 5\tFrequency: 203\n",
      "\tToken: Irish\t\tlength: 5\tFrequency: 232\n",
      "\tToken: Dutch\t\tlength: 5\tFrequency: 297\n",
      "\tToken: Czech\t\tlength: 5\tFrequency: 519\n",
      "\tToken: Korean\t\tlength: 6\tFrequency: 94\n",
      "\tToken: Polish\t\tlength: 6\tFrequency: 139\n",
      "\tToken: French\t\tlength: 6\tFrequency: 277\n",
      "\tToken: German\t\tlength: 6\tFrequency: 724\n",
      "\tToken: Arabic\t\tlength: 6\tFrequency: 2000\n",
      "\tToken: Chinese\t\tlength: 7\tFrequency: 268\n"
     ]
    }
   ],
   "source": [
    "vocab.print_statistics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
